{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDPd6Zo2LMlf",
        "outputId": "896a4e3b-fc80-4ad5-d2db-b68e5f1b25ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# Install Dependencies\n",
        "!pip install transformers torch datasets kaggle scikit-learn seqeval -q\n",
        "print(\"Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6FYkiF9LMll",
        "outputId": "2736378a-b541-4c80-a035-778291562401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cp: cannot stat '/content/drive/MyDrive/kaggle.json': No such file or directory\n",
            "✓ Google Drive mounted\n",
            "✓ Kaggle credentials configured\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and Setup Kaggle\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setup Kaggle credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"✓ Google Drive mounted\")\n",
        "print(\"✓ Kaggle credentials configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5EDOYIEMnxG"
      },
      "outputs": [],
      "source": [
        "# Setup Kaggle API credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amW_AzO4LMlo",
        "outputId": "a1da96dd-b25f-4af3-912b-03d7fed3925f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/urbikn/sroie-datasetv2\n",
            "License(s): other\n",
            "sroie-datasetv2.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Dataset structure:\n",
            "/content/sroie_data\n",
            "/content/sroie_data/SROIE2019\n",
            "/content/sroie_data/SROIE2019/layoutlm-base-uncased\n",
            "/content/sroie_data/SROIE2019/train\n",
            "/content/sroie_data/SROIE2019/train/entities\n",
            "/content/sroie_data/SROIE2019/train/img\n",
            "/content/sroie_data/SROIE2019/train/box\n",
            "/content/sroie_data/SROIE2019/test\n",
            "/content/sroie_data/SROIE2019/test/entities\n",
            "/content/sroie_data/SROIE2019/test/img\n",
            "/content/sroie_data/SROIE2019/test/box\n",
            "\n",
            "✓ SROIE dataset downloaded and extracted\n"
          ]
        }
      ],
      "source": [
        "# Clean up old data first\n",
        "!rm -rf /content/sroie_data\n",
        "\n",
        "# Download SROIE dataset\n",
        "!kaggle datasets download -d urbikn/sroie-datasetv2\n",
        "\n",
        "# Extract with overwrite\n",
        "!unzip -o -q sroie-datasetv2.zip -d /content/sroie_data\n",
        "\n",
        "# Check structure\n",
        "print(\"Dataset structure:\")\n",
        "!find /content/sroie_data -type d | head -20\n",
        "\n",
        "print(\"\\n✓ SROIE dataset downloaded and extracted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOcSOkgfLMlq",
        "outputId": "2aad1633-0f77-4083-893e-6eb6b3fe7a3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import LayoutLMTokenizerFast, LayoutLMForTokenClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toYJpXrWLMlt",
        "outputId": "829a438b-adca-464f-9bc8-cd8bf76a069a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels: ['O', 'B-COMPANY', 'I-COMPANY', 'B-DATE', 'I-DATE', 'B-ADDRESS', 'I-ADDRESS', 'B-TOTAL', 'I-TOTAL']\n"
          ]
        }
      ],
      "source": [
        "# Define Labels\n",
        "# BIO tagging scheme\n",
        "ENTITY_LABELS = [\n",
        "    'O',\n",
        "    'B-COMPANY',\n",
        "    'I-COMPANY',\n",
        "    'B-DATE',\n",
        "    'I-DATE',\n",
        "    'B-ADDRESS',\n",
        "    'I-ADDRESS',\n",
        "    'B-TOTAL',\n",
        "    'I-TOTAL'\n",
        "]\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(ENTITY_LABELS)}\n",
        "id2label = {i: label for i, label in enumerate(ENTITY_LABELS)}\n",
        "\n",
        "print(f\"Labels: {ENTITY_LABELS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5ZfCkVwLMlu",
        "outputId": "cb13f7e6-7fbd-4ed0-a11f-12f31ae1dafa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 626 receipts from train set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 626/626 [00:00<00:00, 669.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Loaded 626 training receipts\n",
            "\n",
            "First receipt preview:\n",
            "Words: ['TAN WOON YANN', 'BOOK TA .K(TAMAN DAYA) SDN BND', '789417-W', 'NO.53 55,57 & 59, JALAN SAGU 18,', 'TAMAN DAYA,', '81100 JOHOR BAHRU,', 'JOHOR.', 'DOCUMENT NO : TD01167104', 'DATE:', '25/12/2018 8:13:39 PM']\n",
            "Bboxes: [[147, 23, 668, 59], [102, 76, 902, 112], [420, 112, 584, 129], [225, 133, 785, 151], [394, 157, 613, 174]]\n",
            "Labels: ['O', 'O', 'O', 'B-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'O', 'O', 'O']\n",
            "\n",
            "Label distribution:\n",
            "  B-ADDRESS      :   610\n",
            "  B-COMPANY      :   481\n",
            "  B-DATE         :   179\n",
            "  B-TOTAL        :  1594\n",
            "  I-ADDRESS      :  1013\n",
            "  I-COMPANY      :    82\n",
            "  I-DATE         :   117\n",
            "  I-TOTAL        :    47\n",
            "  O              : 29503\n",
            "\n",
            "✓ B- tags found: 2864 (good if > 0!)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Parse SROIE Dataset with Bounding Boxes\n",
        "def normalize_bbox(bbox, width=1000, height=1000):\n",
        "    \"\"\"Normalize bounding box to 0-1000 scale (LayoutLM format).\"\"\"\n",
        "    x_coords = [bbox[0], bbox[2], bbox[4], bbox[6]]\n",
        "    y_coords = [bbox[1], bbox[3], bbox[5], bbox[7]]\n",
        "\n",
        "    x_min = min(x_coords)\n",
        "    y_min = min(y_coords)\n",
        "    x_max = max(x_coords)\n",
        "    y_max = max(y_coords)\n",
        "\n",
        "    return [\n",
        "        int((x_min / width) * 1000),\n",
        "        int((y_min / height) * 1000),\n",
        "        int((x_max / width) * 1000),\n",
        "        int((y_max / height) * 1000)\n",
        "    ]\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normalize text for matching\"\"\"\n",
        "    return re.sub(r'[^a-zA-Z0-9]+', '', text.lower())\n",
        "\n",
        "def assign_bio_labels(words, entities):\n",
        "    \"\"\"\n",
        "    Assign proper BIO labels to words based on entity annotations.\n",
        "\n",
        "    IMPROVED: Uses proper B- and I- tagging with sequence matching.\n",
        "    \"\"\"\n",
        "    # Normalize entity values\n",
        "    entity_normalized = {\n",
        "        'company': normalize_text(entities.get('company', '')),\n",
        "        'date': normalize_text(entities.get('date', '')),\n",
        "        'address': normalize_text(entities.get('address', '')),\n",
        "        'total': normalize_text(entities.get('total', ''))\n",
        "    }\n",
        "\n",
        "    labels = ['O'] * len(words)\n",
        "\n",
        "    # For each entity type, find matching sequences\n",
        "    for entity_type in ['company', 'date', 'address', 'total']:\n",
        "        entity_val = entity_normalized[entity_type]\n",
        "        if not entity_val:\n",
        "            continue\n",
        "\n",
        "        # Normalize all words\n",
        "        words_norm = [normalize_text(w) for w in words]\n",
        "\n",
        "        # Try to find the entity as a sequence of words\n",
        "        i = 0\n",
        "        while i < len(words):\n",
        "            # Check if current position starts the entity\n",
        "            matched_length = 0\n",
        "            entity_pos = 0\n",
        "            temp_i = i\n",
        "\n",
        "            # Try to match entity tokens sequentially\n",
        "            while temp_i < len(words) and entity_pos < len(entity_val):\n",
        "                word_norm = words_norm[temp_i]\n",
        "                if not word_norm:\n",
        "                    temp_i += 1\n",
        "                    continue\n",
        "\n",
        "                # Check if this word is part of the entity\n",
        "                if word_norm in entity_val[entity_pos:]:\n",
        "                    matched_length += 1\n",
        "                    entity_pos += len(word_norm)\n",
        "                    temp_i += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            # If we matched significant portion of entity, label it\n",
        "            if matched_length > 0 and entity_pos / len(entity_val) > 0.5:\n",
        "                # Assign B- to first token, I- to rest\n",
        "                labels[i] = f'B-{entity_type.upper()}'\n",
        "                for j in range(i + 1, i + matched_length):\n",
        "                    labels[j] = f'I-{entity_type.upper()}'\n",
        "                i = i + matched_length\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "    return labels\n",
        "\n",
        "def parse_sroie_receipt(box_file, entity_file):\n",
        "    \"\"\"Parse SROIE receipt with proper BIO labels.\"\"\"\n",
        "    # Read entity annotations\n",
        "    with open(entity_file, 'r', encoding='utf-8') as f:\n",
        "        entities = json.load(f)\n",
        "\n",
        "    words = []\n",
        "    bboxes = []\n",
        "\n",
        "    # First pass: get image dimensions\n",
        "    max_x, max_y = 0, 0\n",
        "    with open(box_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            if len(parts) >= 8:\n",
        "                coords = [int(parts[i]) for i in range(8)]\n",
        "                max_x = max(max_x, max(coords[0], coords[2], coords[4], coords[6]))\n",
        "                max_y = max(max_y, max(coords[1], coords[3], coords[5], coords[7]))\n",
        "\n",
        "    img_width = max_x * 1.1\n",
        "    img_height = max_y * 1.1\n",
        "\n",
        "    # Second pass: parse tokens with bboxes\n",
        "    with open(box_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            parts = line.split(',')\n",
        "            if len(parts) < 9:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                coords = [int(parts[i]) for i in range(8)]\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            word = ','.join(parts[8:]).strip()\n",
        "            if not word:\n",
        "                continue\n",
        "\n",
        "            bbox = normalize_bbox(coords, img_width, img_height)\n",
        "\n",
        "            words.append(word)\n",
        "            bboxes.append(bbox)\n",
        "\n",
        "    # Assign BIO labels using improved algorithm\n",
        "    labels = assign_bio_labels(words, entities)\n",
        "\n",
        "    return words, bboxes, labels\n",
        "\n",
        "def load_sroie_dataset(data_dir, split='train'):\n",
        "    \"\"\"Load SROIE dataset with improved BIO labeling.\"\"\"\n",
        "    split_dir = Path(data_dir) / split\n",
        "    box_dir = split_dir / 'box'\n",
        "    entity_dir = split_dir / 'entities'\n",
        "\n",
        "    all_words = []\n",
        "    all_bboxes = []\n",
        "    all_labels = []\n",
        "\n",
        "    box_files = sorted(box_dir.glob('*.txt'))\n",
        "\n",
        "    print(f\"Loading {len(box_files)} receipts from {split} set...\")\n",
        "\n",
        "    for box_file in tqdm(box_files):\n",
        "        entity_file = entity_dir / box_file.name\n",
        "\n",
        "        if not entity_file.exists():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            words, bboxes, labels = parse_sroie_receipt(box_file, entity_file)\n",
        "\n",
        "            if len(words) > 0:\n",
        "                all_words.append(words)\n",
        "                all_bboxes.append(bboxes)\n",
        "                all_labels.append(labels)\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing {box_file.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_words, all_bboxes, all_labels\n",
        "\n",
        "# Load training data\n",
        "data_dir = '/content/sroie_data/SROIE2019'\n",
        "train_words, train_bboxes, train_labels = load_sroie_dataset(data_dir, 'train')\n",
        "\n",
        "print(f\"\\n✓ Loaded {len(train_words)} training receipts\")\n",
        "\n",
        "if len(train_words) > 0:\n",
        "    print(f\"\\nFirst receipt preview:\")\n",
        "    print(f\"Words: {train_words[0][:10]}\")\n",
        "    print(f\"Bboxes: {train_bboxes[0][:5]}\")\n",
        "    print(f\"Labels: {train_labels[0][:10]}\")\n",
        "\n",
        "    # Label distribution\n",
        "    all_train_labels = [label for receipt_labels in train_labels for label in receipt_labels]\n",
        "    label_counts = Counter(all_train_labels)\n",
        "    print(f\"\\nLabel distribution:\")\n",
        "    for label, count in sorted(label_counts.items()):\n",
        "        print(f\"  {label:15s}: {count:5d}\")\n",
        "\n",
        "    # Check for B- tags\n",
        "    b_tags = sum(1 for label in all_train_labels if label.startswith('B-'))\n",
        "    print(f\"\\n✓ B- tags found: {b_tags} (good if > 0!)\")\n",
        "else:\n",
        "    print(\"\\n⚠ WARNING: No receipts loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV_i5GcTLMlx",
        "outputId": "6adcd832-fab0-40fb-bad3-99a4050197d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ LayoutLM Dataset class defined\n"
          ]
        }
      ],
      "source": [
        "# Create LayoutLM Dataset\n",
        "class LayoutLMDataset(Dataset):\n",
        "    def __init__(self, words_list, bboxes_list, labels_list, tokenizer, max_length=512):\n",
        "        self.words_list = words_list\n",
        "        self.bboxes_list = bboxes_list\n",
        "        self.labels_list = labels_list\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words = self.words_list[idx]\n",
        "        bboxes = self.bboxes_list[idx]\n",
        "        labels = self.labels_list[idx]\n",
        "\n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            words,\n",
        "            is_split_into_words=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Align labels and bboxes with subword tokens\n",
        "        word_ids = encoding.word_ids()\n",
        "        aligned_labels = []\n",
        "        aligned_bboxes = []\n",
        "        previous_word_idx = None\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # Special tokens: [CLS], [SEP], [PAD]\n",
        "                aligned_labels.append(-100)\n",
        "                aligned_bboxes.append([0, 0, 0, 0])  # Zero bbox for special tokens\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # First subword of a word\n",
        "                aligned_labels.append(label2id[labels[word_idx]])\n",
        "                aligned_bboxes.append(bboxes[word_idx])\n",
        "            else:\n",
        "                # Continuation subword\n",
        "                aligned_labels.append(-100)\n",
        "                aligned_bboxes.append(bboxes[word_idx])  # Reuse same bbox\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'bbox': torch.tensor(aligned_bboxes, dtype=torch.long),\n",
        "            'labels': torch.tensor(aligned_labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"✓ LayoutLM Dataset class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Swsp7xmLMly",
        "outputId": "84c587d5-0554-499e-ba06-d86e3faf7fb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Train samples: 563\n",
            "✓ Val samples: 63\n",
            "✓ Train batches: 71\n",
            "✓ Val batches: 8\n"
          ]
        }
      ],
      "source": [
        "# Prepare Data Loaders\n",
        "# Initialize LayoutLM tokenizer\n",
        "tokenizer = LayoutLMTokenizerFast.from_pretrained('microsoft/layoutlm-base-uncased')\n",
        "\n",
        "# Split train/val\n",
        "train_words_split, val_words, train_bboxes_split, val_bboxes, train_labels_split, val_labels = train_test_split(\n",
        "    train_words, train_bboxes, train_labels, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LayoutLMDataset(train_words_split, train_bboxes_split, train_labels_split, tokenizer)\n",
        "val_dataset = LayoutLMDataset(val_words, val_bboxes, val_labels, tokenizer)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "print(f\"✓ Train samples: {len(train_dataset)}\")\n",
        "print(f\"✓ Val samples: {len(val_dataset)}\")\n",
        "print(f\"✓ Train batches: {len(train_loader)}\")\n",
        "print(f\"✓ Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXCgyeVELMlz",
        "outputId": "6705023a-9645-4cc2-8544-f3413d594347"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ LayoutLM model initialized with 112,634,889 parameters\n",
            "✓ Training for 10 epochs\n",
            "✓ Total steps: 710\n"
          ]
        }
      ],
      "source": [
        "#Initialise LayoutLM Model\n",
        "# Load LayoutLM for token classification\n",
        "model = LayoutLMForTokenClassification.from_pretrained(\n",
        "    'microsoft/layoutlm-base-uncased',\n",
        "    num_labels=len(ENTITY_LABELS),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Training hyperparameters\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 5e-5\n",
        "WARMUP_STEPS = 200\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=WARMUP_STEPS,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(f\"✓ LayoutLM model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "print(f\"✓ Training for {EPOCHS} epochs\")\n",
        "print(f\"✓ Total steps: {total_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0MIieN2LMl0",
        "outputId": "b22eadd5-e7a9-46aa-c2db-573dda1ac824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Epoch 1/10\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 71/71 [00:14<00:00,  4.92it/s, loss=0.3073]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 1.2081\n",
            "Val F1 Score: 0.3953\n",
            "✓ New best model saved (F1: 0.3953)\n",
            "\n",
            "============================================================\n",
            "Epoch 2/10\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 71/71 [00:14<00:00,  4.92it/s, loss=0.3030]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1785\n",
            "Val F1 Score: 0.8658\n",
            "✓ New best model saved (F1: 0.8658)\n",
            "\n",
            "============================================================\n",
            "Epoch 3/10\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 71/71 [00:14<00:00,  4.92it/s, loss=0.0379]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0831\n",
            "Val F1 Score: 0.8818\n",
            "✓ New best model saved (F1: 0.8818)\n",
            "\n",
            "============================================================\n",
            "Epoch 4/10\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 71/71 [00:14<00:00,  4.93it/s, loss=0.0082]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0542\n",
            "Val F1 Score: 0.9204\n",
            "✓ New best model saved (F1: 0.9204)\n",
            "\n",
            "============================================================\n",
            "Epoch 5/10\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 71/71 [00:14<00:00,  4.93it/s, loss=0.0054]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0337\n",
            "Val F1 Score: 0.8985\n",
            "\n",
            "============================================================\n",
            "Epoch 6/10\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 71/71 [00:14<00:00,  4.94it/s, loss=0.1245]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0258\n",
            "Val F1 Score: 0.9184\n",
            "\n",
            "============================================================\n",
            "Epoch 7/10\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 71/71 [00:14<00:00,  4.94it/s, loss=0.0027]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0182\n",
            "Val F1 Score: 0.9244\n",
            "✓ New best model saved (F1: 0.9244)\n",
            "\n",
            "============================================================\n",
            "Epoch 8/10\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 71/71 [00:14<00:00,  4.93it/s, loss=0.0021]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0131\n",
            "Val F1 Score: 0.9075\n",
            "\n",
            "============================================================\n",
            "Epoch 9/10\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 71/71 [00:14<00:00,  4.94it/s, loss=0.0419]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0103\n",
            "Val F1 Score: 0.9220\n",
            "\n",
            "============================================================\n",
            "Epoch 10/10\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 71/71 [00:14<00:00,  4.94it/s, loss=0.0010]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0078\n",
            "Val F1 Score: 0.9174\n",
            "\n",
            "============================================================\n",
            "Training completed!\n",
            "Best validation F1: 0.9244\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "def evaluate(model, data_loader):\n",
        "    \"\"\"Evaluate model on validation set\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            bbox = batch['bbox'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                bbox=bbox\n",
        "            )\n",
        "\n",
        "            predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "            # Convert to labels\n",
        "            for i in range(len(predictions)):\n",
        "                pred_labels = []\n",
        "                true_labels = []\n",
        "\n",
        "                for j in range(len(predictions[i])):\n",
        "                    if labels[i][j] != -100:\n",
        "                        pred_labels.append(id2label[predictions[i][j].item()])\n",
        "                        true_labels.append(id2label[labels[i][j].item()])\n",
        "\n",
        "                all_predictions.append(pred_labels)\n",
        "                all_true_labels.append(true_labels)\n",
        "\n",
        "    f1 = f1_score(all_true_labels, all_predictions)\n",
        "    return f1, all_predictions, all_true_labels\n",
        "\n",
        "# Training loop\n",
        "best_f1 = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc='Training')\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        bbox = batch['bbox'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            bbox=bbox,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    val_f1, _, _ = evaluate(model, val_loader)\n",
        "\n",
        "    print(f\"\\nTrain Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Val F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        torch.save(model.state_dict(), '/content/best_layoutlm_model.pt')\n",
        "        print(f\"✓ New best model saved (F1: {best_f1:.4f})\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training completed!\")\n",
        "print(f\"Best validation F1: {best_f1:.4f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXv6fm35LMl1",
        "outputId": "46288e31-eee7-4596-ffdd-f68c128829bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     ADDRESS       0.83      0.89      0.86        61\n",
            "     COMPANY       0.86      0.93      0.90        46\n",
            "        DATE       1.00      1.00      1.00        22\n",
            "       TOTAL       0.94      0.97      0.95       149\n",
            "\n",
            "   micro avg       0.90      0.95      0.92       278\n",
            "   macro avg       0.91      0.95      0.93       278\n",
            "weighted avg       0.90      0.95      0.92       278\n",
            "\n",
            "\n",
            "Overall F1 Score: 0.9244\n"
          ]
        }
      ],
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load('/content/best_layoutlm_model.pt'))\n",
        "model.eval()\n",
        "\n",
        "# Full validation evaluation\n",
        "val_f1, val_predictions, val_true_labels = evaluate(model, val_loader)\n",
        "\n",
        "print(\"\\nValidation Set Performance:\")\n",
        "print(classification_report(val_true_labels, val_predictions))\n",
        "\n",
        "print(f\"\\nOverall F1 Score: {val_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Zk5alzLMl4",
        "outputId": "c4bd1178-9b5c-46e1-a2a7-72f89f7238bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model saved to /content/drive/MyDrive/sroie_layoutlm_final\n",
            "\n",
            "Files saved:\n",
            "  - pytorch_model.bin\n",
            "  - config.json\n",
            "  - tokenizer files\n"
          ]
        }
      ],
      "source": [
        "# save model for deployment\n",
        "import os\n",
        "\n",
        "# Create output directory\n",
        "output_dir = '/content/drive/MyDrive/ner_layoutlm_final'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Save config\n",
        "config = {\n",
        "    'num_labels': len(ENTITY_LABELS),\n",
        "    'id2label': id2label,\n",
        "    'label2id': label2id,\n",
        "    'max_length': 512,\n",
        "    'model_type': 'layoutlm'\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_dir, 'config.json'), 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(f\"✓ Model saved to {output_dir}\")\n",
        "print(\"\\nFiles saved:\")\n",
        "print(\"  - pytorch_model.bin\")\n",
        "print(\"  - config.json\")\n",
        "print(\"  - tokenizer files\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
