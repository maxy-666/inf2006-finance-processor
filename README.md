
INF2006 Project ‚Äì Cloud-Native Financial Document Processor (AIaaS)

A cloud-native AI-as-a-Service (AIaaS) platform that automates financial document processing (invoices, receipts) through a multi-stage machine learning pipeline. The system performs OCR, entity extraction, validation, auto-categorisation, and Big Data analytics. All infrastructure is deployed using Terraform, and data analytics are powered by Apache Spark on EMR.

Features
Cloud-Native Architecture

Fully serverless ingestion pipeline (API Gateway, Lambda, Step Functions, EventBridge)

Robust workflow control with retry logic and fault tolerance

AI Document Processing

OCR via Amazon Textract

Entity extraction using a custom LayoutLM model on SageMaker

Extracted results stored in DynamoDB

Big Data Lakehouse

Decoupled storage and compute using Amazon S3 and EMR

Data Lake built with JSON Lines, Parquet, and Delta optimisation

Analytics and Machine Learning

Batch ETL processing with Apache Spark

Spend aggregation and category analytics

Random Forest model for category prediction using Spark MLlib

Automated chart generation (Matplotlib)

Interactive dashboards using Amazon QuickSight with Athena

Infrastructure as Code

Entire system deployed through Terraform

Architecture Overview
1. Operational Pipeline (Real-Time)

Handles incoming financial documents and performs immediate extraction.

Flow:
API Gateway ‚Üí S3 (Presigned Upload) ‚Üí Step Functions ‚Üí Textract ‚Üí LayoutLM (SageMaker) ‚Üí DynamoDB

Components:

API Gateway for secure presigned uploads

Lambda for orchestration logic

Step Functions to coordinate OCR and extraction

Textract for high-accuracy OCR

Custom LayoutLM model on SageMaker

DynamoDB to store processing results

2. Analytics Pipeline (Batch Big Data)

Processes historical data and enables analytics and ML training.

Flow:
DynamoDB Streams ‚Üí Lambda ETL ‚Üí S3 Data Lake ‚Üí EMR Spark ‚Üí Athena ‚Üí QuickSight

Components:

DynamoDB Streams for change data capture

Lambda to ingest raw documents into S3

S3 as a scalable Data Lake (partitioned storage)

Apache Spark jobs for ETL, aggregation, and machine learning

Athena + QuickSight for business intelligence

Technology Stack
Category	Technology	Purpose
Cloud Provider	AWS	Infrastructure and managed services
IaC	Terraform	Provisioning and configuration
Orchestration	Step Functions, Lambda, SQS	Workflow and serverless compute
AI/ML	Textract, SageMaker, Spark MLlib	OCR, entity extraction, model training
Big Data	Apache Spark (PySpark)	ETL, analytics, ML
Frontend	HTML/JavaScript with Node.js proxy	UI and secure API communication
Deployment Guide
Prerequisites

AWS CLI configured with Administrator permissions

Terraform v1.0+

Node.js v16+

Python 3.9

Phase 1: Deploy Infrastructure (Terraform)
cd backend-iac
terraform init
terraform apply -auto-approve


Record the generated Terraform output values:

API Gateway base URL

Document Upload S3 Bucket name

Analytics Data Lake S3 Bucket name

Phase 2: Post-Deployment Configuration
1. Upload Spark Scripts to S3

Replace [DOCS_BUCKET_NAME] accordingly.

aws s3 cp ../backend-spark/batch_analytics_safe.py s3://[DOCS_BUCKET_NAME]/scripts/
aws s3 cp ../backend-spark/spark_ml_analytics.py s3://[DOCS_BUCKET_NAME]/scripts/

2. Configure Backend Proxy (Node.js)
cd ../backend-proxy


Create .env:

PORT=3000
API_GATEWAY_URL=https://[API_ID].execute-api.us-east-1.amazonaws.com/generate-upload-url
DASHBOARD_API_URL=https://[API_ID].execute-api.us-east-1.amazonaws.com/get-dashboard-url


Start the server:

npm install
node server.js

3. Set Up QuickSight Dashboard

Run AWS Glue Crawler: inf2006-analytics-crawler

In QuickSight, create a dataset using Athena

Select database inf2006_analytics_db

Select table category_summary_v3

Build a visual (bar chart) and publish as a dashboard

If needed, update the Dashboard ID in Lambda environment variables

Running the Pipelines
1. Ingestion (Upload Documents)

Open:

frontend/index.html


Upload receipts or invoices.

Verify execution in AWS Step Functions:
OCR ‚Üí Entity Extraction ‚Üí Database Save

2. Run Big Data Analytics Job (EMR)

SSH into EMR:

ssh -i path/to/project-key.pem hadoop@ec2-xx-xx-xx.compute-1.amazonaws.com


Run ETL:

spark-submit s3://[DOCS_BUCKET_NAME]/scripts/batch_analytics_safe.py

3. Machine Learning (Training)
spark-submit s3://[DOCS_BUCKET_NAME]/scripts/spark_ml_analytics.py


Expected output example:

Accuracy: 0.96

4. Visualisation

Static chart generated by Spark will appear in the frontend

Interactive QuickSight dashboard loads via the backend proxy

5. Reproduction and Deployment Instruments (Terraform)
These steps will reproduce the full infrastructure environment using **Terraform**

5.1 Prerequisites:
Terraform v1.13.3 Windows
Cloud provider CLI Installed (AWS CLI)
Github Desktop with this repository loaded
Visual Studio Code

5.2 Configure AWS CLI
Sign into the AWS Management Console
Navigate to the IAM dashboard
Choose users from the left navigation panel
select the user to create access keys
Go to the security credentials tab
Under the access keys, create access key
Select Command Line Interface (CLI) as the use case and proceed
Note down the access key ID generated

With AWS CLI installed, enter terminal or command prompt and run:
"aws configure"
AWS CLI will prompt for the access key ID and just paste it in

5.3 Terraform instructions
Using Github Desktop, open up this repository using Visual Studio Code
navigate to /main/backend-iac/
Open the repository via Visual Studio Code
In the control terminal in Visual Studio Code, navigate to the backend-iac directory with cd
Run terraform with the following commands:

"terraform init"
"terraform validate"
"terraform plan"
"terraform apply"

Following these steps should apply the component configurations from the respective .tf files into the AWS account which you had linked via the access key ID

## üõ†Ô∏è Technology Stack
| Category | Technology | Purpose |
| :--- | :--- | :--- |
| **Cloud Provider** | AWS (Amazon Web Services) | Infrastructure hosting and managed services. |
| **IaC** | Terraform | Defining and deploying all AWS infrastructure (IaC). |
| **Orchestration** | AWS Lambda, SQS, Step Functions | Serverless computing and workflow management. |
| **Frontend** | [e.g., React, or plain HTML/JS] | User interface for document upload and result viewing. |
| **Key AI Service** | AWS Textract | Managed service for high-accuracy OCR. |

## üë• Team & Contributions
| Name | Role | Core Responsibility |
| :--- | :--- | :--- |
| **[Max Tan]** | Cloud Architect & Front-End Developer | IaC (Terraform), API Gateway, Lambda integration |
| **[Loh Kai Chuin]** | Cloud Architect & Front-End Developer | Website Frontend, IaC (Terraform) for Frontend, UI/UX. |
| **[Nurul Zahirah Binte Muhamadnoh]** | ML Developer | Model 2: Entity Extraction (LayoutLM), BIO Tagging & Label Alignment |
| **[Linus Koh Jiang Zhen]** | ML Developer | Model 2: Entity Extraction, Model 3: Expense Categorization Development |
| **[Wong Li Shen]** | ML Engineer | Machine Learning Model Evaluation (Azure Document Intelligence and AWS Textract), OCR Model 1|

---
